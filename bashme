#!/usr/bin/env python3
import os
import logging
from transformers import AutoTokenizer, pipeline, logging
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig


logging.set_verbosity(logging.CRITICAL)

def load_CodeLlama_llm():
    model_name_or_path = os.getenv("MPSD_CODE_LLAMA",None) or "../models/CodeLlama-7B-Instruct-GPTQ"

    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)

    model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,
            use_safetensors=True,
            trust_remote_code=True,
            device="cuda:0",
            use_triton=use_triton,
            quantize_config=None)

    return model

def tok

def main():

    # load the model
    # set the prompt
    # query the prompt from user input
    # parse the results
    # print the results
    pass


if __name__ == '__main__':
    main()